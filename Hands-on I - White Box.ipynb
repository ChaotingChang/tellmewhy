{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter I :  White Box Techniques ( ~ 30 minutes) \n",
    "\n",
    "Guideline:  \n",
    " \n",
    "- Prerequisites: Data Preparation (Understand and load the data) \n",
    "- Decision Rules\n",
    "    - Building decision rules\n",
    "    - OneR \n",
    "    - ZeroR \n",
    "- Decision Trees \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries:  Understand the problem and load the data\n",
    "This dataset contains information about survival passengers of titanic. The objective of the task is to classify if a passenger survives or not according to the features. \n",
    "\n",
    "The dataset is available in folder: data/titanic.xls\n",
    "\n",
    "Attribute Information (in order):\n",
    "    - pclass     Passenger class \n",
    "    - name       Name\n",
    "    - sex        Sex \n",
    "    - age        Age\n",
    "    - sibsp      Number of Siblings/Spouses Aboard\n",
    "    - parch      Number of Parents/Children Aboard\n",
    "    - ticket     Ticket Number\n",
    "    - fare       Passenger Fare\n",
    "    - cabin      Cabin \n",
    "    - embarked   Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "    - boat       Lifeboat (if survived)\n",
    "\n",
    "Target Variable \n",
    "    - survided     The passenger survived or not \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def metrics(y_test: list, y_pred: list):\n",
    "    print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "    print(\"Recall  : {}\".format(recall_score(y_test, y_pred)))\n",
    "    print(\"F1-Score: {}\".format(f1_score(y_test, y_pred)))\n",
    "\n",
    "\n",
    "#Read Data from source \n",
    "data_df = pd.read_excel(\"Data/titanic_dataset.xls\").set_index(\"name\")\n",
    "\n",
    "# Split the data into train and test set \n",
    "x_train, x_test, y_train, y_test = train_test_split(data_df.iloc[:, :-1], data_df[\"survived\"], \n",
    "                                                    test_size=0.25, random_state=4242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Rules\n",
    "Decision rules are a set of IF-THEN rules. The combination of several rules can be used to make predictions. The rules can be defined manually by the user (if the user has good domain knowledge it can encode very good rules). \n",
    "\n",
    "An IF-THEN rule is an expression of the form: \n",
    "```\n",
    "IF condition THEN conclusion\n",
    "\n",
    "# Example: \n",
    "IF age=youth AND love_coffe=YES AND favourite_meetup=\"Python Barcelona\"  \n",
    "        THEN uses_python=YES\n",
    "ELSE uses_python=NO\n",
    "\n",
    "```\n",
    "In this section is proposed to think and build your own decision rules set (just making some hypothesis about the dataset). Later in this section are explained two simple algorithms to learn rules from the dataset. To evaluate Decision rules are commonly used the following metrics:  Coverage (percentage of instances which the condition apply) and Accuracy (percentage of correct instances)\n",
    "<div>\n",
    "    <center> <img src=\"Data/img/coverage_and_accuracy.JPG\" width=\"250\" /> </center>\n",
    "</div>\n",
    "\n",
    "|D| number of observations on Dataset, Ncovers : covered observations by the rules and Ncorrect: number of observations correctly classified\n",
    "\n",
    "\n",
    "\n",
    "### TODO\n",
    "\n",
    "A) Analyze the dataset and encode an IF-ELSE block on the training set  \n",
    "        \n",
    "B) Calculate the coverage of the rule in training set\n",
    "\n",
    "C) Calculate the accuracy of the rule in the training set\n",
    "\n",
    "D) Filter and build y_pred (an array with the class binary probability of the input samples using the test set using your IF-ELSE condition\n",
    "        1 if the passenger survived \n",
    "        0 if not survived \n",
    "\n",
    "E) Evaluate on test set using metrics() funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dr = x_train.copy()\n",
    "# Analyze the dataset and encode an IF-ELSE block on training set \n",
    "x_train_dr[\"index\"] = range(0, x_train_dr.shape[0]) \n",
    "\n",
    "# IF EMBARKED = Q THEN survived = YES \n",
    "# ELSE survived = NO \n",
    "x_train_dr = x_train_dr[x_train_dr[\"embarked\"] == \"Q\"]\n",
    "\n",
    "# @TODO: Try to build your own rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the coverage of the rule \n",
    "print(\"Coverage on training set {}\".format(x_train_dr.shape[0] / x_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of the rule\n",
    "original_tags = y_train[x_train_dr[\"index\"]]\n",
    "n_correct = sum(np.ones(original_tags.shape[0])*original_tags)\n",
    "print (\"Accuracy on training set: {}\".format(n_correct / x_train_dr.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build y_pred using previous rule \n",
    "x_test_dr = x_test.copy()\n",
    "\n",
    "\n",
    "# Evaluate the rule\n",
    "metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneR Algorithm  \n",
    "OneR probably is one of the simplest methods for classification (for discrete attributes)  due to the simplicity we can quickly explain each prediction.  Although the simplicity of this algorithm it is only a few percentage points less accurate than decision trees (source: Very Simple Classification Rules Perform Well on Most Commonly Used Datasets [link](https://www.mlpack.org/papers/ds.pdf)) \n",
    "\n",
    "OneR works as follows: \n",
    "\n",
    "```\n",
    "For feature in the dataset: \n",
    "     We build a Frequency table \n",
    "         - 1. Count how often each value of target appears in category groups\n",
    "         - 2. Encode the frequency class into a rule\n",
    "         - 3. Calculate the quality of the rule \n",
    "\n",
    "The best predictor is chosen as the one with the smallest error\n",
    "\n",
    "\n",
    "```\n",
    "Read more about OneR Algorithm at the following [link](https://www.saedsayad.com/oner.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Count how often each value of target appears \n",
    "# 2. Find the most frequent class \n",
    "pd.concat([x_train, pd.Series(y_train, name=\"survived\")], axis = 1) \\\n",
    "  .groupby(['embarked','survived']) \\\n",
    "  .size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Make the rule assigning that class to this value\n",
    "# Using the previous frequency we can determine the following rules: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "IF embarked = C THEN survived = YES\n",
    "IF embarked = Q THEN survived = NO\n",
    "IF embarked = S THEN survived = NO\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codify previous rules\n",
    "rule_codification = {\"C\": 1, \"Q\": 0, \"S\": 0}\n",
    "y_pred = x_test['embarked'].map(rule_codification)\n",
    "\n",
    "# Evaluate the rule\n",
    "metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO \n",
    "\n",
    "A) Build a Frequency table for sex feature \n",
    "\n",
    "B) Encode the Frequency table to a rule  \n",
    "\n",
    "C) Compare the embarked predictor with sex predictor, better or worst? \n",
    "\n",
    "D) Try ZeroR,  even simplest classification method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:  Frequency table using sex feature \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B: Calculate Accuracy on Test set \n",
    "# hint consider using map function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C: Evaluate the rule  \n",
    "metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ZeroR Algorithm \n",
    "ZeroR algorithm is even more simple. This algorithm is based on predicting the majority class, the classifier relies only on the target value and ignores the predictors. \n",
    "Example: imagine a dataset for email spam classification (is_spam) looking at the target value we found 57 cases of SPAM and 20 of NO SPAM. ZeroR builts the following rule: is_spam(X) = YES, in other words: for all predicted instances is returned YES. \n",
    "Of course, the limitations of ZeroR and OneR are obvious but these two algorithms can be used as a useful baseline for Machine Learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurences on train set (y_test) and determine mayority class (Survived or NOT). \n",
    "# Hint: Use unique function from numpy and zip \n",
    "\n",
    "\n",
    "# Evaluate the rule using y_test\n",
    "metrics(y_test, np.zeros(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "Not too much to explain .. decision Trees is one of the top popular supervised Machine Learning methods, it builds a classifier or a regressor model in the form of a tree structure. \n",
    "Decision trees are simple to understand and interpret;  we can easilly print the decision tree or determine the decision path of a prediction.  \n",
    "\n",
    "\n",
    "<div>\n",
    "    <center> <img src=\"Data/img/decision_tree.JPG\" width=\"500\" /> </center>\n",
    "</div>\n",
    "\n",
    "During this section is to show how to output graphical trees (using Graphviz, a Graph visualization Software). The last section includes a function to print the decision path of the decision tree. Decision_path is a function from DecisionTreeClassifier which returns a sparse matric showing which nodes of the tree the prediction goes through,  this information can be used to understand the why of a prediction. \n",
    "\n",
    "### TODO\n",
    "\n",
    "A) Preprocess data. Detect Missing values and encode categorical features using OneHotEncoder. Use the function preprocessing_dataframe()\n",
    "\n",
    "B) Train a decision tree model using x_train and x_test\n",
    "\n",
    "C) Evaluate the model \n",
    "\n",
    "D) Print the tree using graphviz \n",
    "\n",
    "E) Determine the decision path for an observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "def preprocessing_dataframe(data_df: object, missing_values_to_convert : list, categorical_to_encode: list) -> object: \n",
    "    # Handle Missing Values\n",
    "    for feature in missing_values_to_convert: \n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        data_df[feature] = imputer.fit_transform(data_df[[feature]])\n",
    "\n",
    "    # One Hot Encoding\n",
    "    for feature in categorical_to_encode: \n",
    "        onehot_encoding = pd.get_dummies(data_df[feature],prefix=feature)\n",
    "        data_df = pd.concat([data_df, pd.get_dummies(data_df[feature], prefix=feature)],axis=1)\n",
    "        data_df.drop([feature],axis=1, inplace=True)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "# Preprocessing, detect missing vaules and encode categorical features \n",
    "# use the function to preprocessing_dataframe() on x_train and x_test\n",
    "x_train_dtree = preprocessing_dataframe(  ,  , )\n",
    "x_test_dtree = preprocessing_dataframe( ,  , )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Decision Tree using Titanic Dataset (use the implementation from scikit-learn library) \n",
    "dtree = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using metrics funtion (build y_pred and test with y_test) \n",
    "\n",
    "\n",
    "metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "def visualize_tree(model: object, feature_names: list) -> object:\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(model, out_file=dot_data, feature_names = feature_names,\n",
    "                    filled=True, rounded=True, special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    return Image(graph.create_png())\n",
    "\n",
    "# Use the function visualize_tree to print the tree structure\n",
    "# Control de complexity of the three using max_depth\n",
    "visualize_tree(, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decision_path(dtree: object, dataset: object, sample_id: int): \n",
    "    # Adapted from https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html \n",
    "    \n",
    "    node_indicator = dtree.decision_path(x_test_dtree)\n",
    "    feature = dtree.tree_.feature\n",
    "    feature_names = x_test_dtree.columns.values\n",
    "    leave_id = dtree.apply(x_test_dtree)\n",
    "\n",
    "    node_index = node_indicator.indices[node_indicator.indptr[sample_id]: node_indicator.indptr[sample_id + 1]]\n",
    "    threshold = dtree.tree_.threshold\n",
    "    print(\"Decision Path for sample : {} (predicted as {}) \\n\".format(sample_id, dtree.predict([x_test_dtree.iloc[sample_id]])[0]))\n",
    "    for node_id in node_index:\n",
    "        if leave_id[sample_id] == node_id:\n",
    "            continue\n",
    "            \n",
    "        if (x_test_dtree.iloc[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "            threshold_sign = \"<=\"\n",
    "        else:\n",
    "            threshold_sign = \">\"\n",
    "\n",
    "        print(\"decision id node {} : {} (= {}) {} {})\".format(node_id, feature_names[feature[node_id]], \n",
    "                                                              x_test_dtree.iloc[sample_id, feature[node_id]], threshold_sign, threshold[node_id]))\n",
    "\n",
    "        \n",
    "# Use the function print_decision_path to obtain the decision path for an instance (sample_id) in x_test (dataset)\n",
    "print_decision_path( , , )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
